{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import plac\n",
    "import random\n",
    "\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTrainer(object):\n",
    "    \"\"\"\n",
    "    Helperclass to train spacy NER and dependency parser\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        pass\n",
    "    \n",
    "    def train_ner(self, train_data, model=None, new_model_name=\"german_modified\", \n",
    "                  output_dir=None, n_iter=30, labels=None, test_model=False):\n",
    "        \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "        # training data format:\n",
    "        # TRAIN_DATA = [\n",
    "        #     (\n",
    "        #         \"Horses are too tall and they pretend to care about your feelings\",\n",
    "        #         {\"entities\": [(0, 6, LABEL)]},\n",
    "        #     ),\n",
    "        #     (\"Do they bite?\", {\"entities\": []}),\n",
    "        # ]\n",
    "        TRAIN_DATA = train_data\n",
    "        \n",
    "        random.seed(0)\n",
    "        # Add entity recognizer to model if it's not in the pipeline\n",
    "        # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "        if \"ner\" not in nlp.pipe_names:\n",
    "            ner = nlp.create_pipe(\"ner\")\n",
    "            nlp.add_pipe(ner)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "        [ner.add_label(label) for label in labels]  # add new entity label to entity recognizer\n",
    "        optimizer = nlp.resume_training()\n",
    "        move_names = list(ner.move_names)\n",
    "        # get names of other pipes to disable them during training\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        train_losses = []\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "            sizes = compounding(1.0, 4.0, 1.001)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "                losses = {}\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "                # print(\"Losses\", losses)\n",
    "                train_losses.append(losses)\n",
    "\n",
    "        # test the trained model\n",
    "        test_text = \"Do you like horses?\"\n",
    "        doc = nlp(test_text)\n",
    "        print(\"Entities in '%s'\" % test_text)\n",
    "        displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "        # save model to output directory\n",
    "        if output_dir is not None:\n",
    "            output_dir = Path(output_dir)\n",
    "            if not output_dir.exists():\n",
    "                output_dir.mkdir()\n",
    "            nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "            nlp.to_disk(output_dir)\n",
    "            print(\"Saved model to: \", output_dir)\n",
    "            \n",
    "            if test_model:\n",
    "                # test the saved model\n",
    "                print(\"Loading from\", output_dir)\n",
    "                nlp2 = spacy.load(output_dir)\n",
    "                # Check the classes have loaded back consistently\n",
    "                assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "                doc2 = nlp2(test_text)\n",
    "                for ent in doc2.ents:\n",
    "                    print(ent.label_, ent.text)\n",
    "                    \n",
    "        return train_losses\n",
    "    \n",
    "    def train_dep(self, train_data, model=None, output_dir=None, n_iter=15, test_model=False):\n",
    "        \"\"\"Load the model, set up the pipeline and train the parser.\"\"\"\n",
    "        # training data format:\n",
    "        # TRAIN_DATA = [\n",
    "        #     (\n",
    "        #         \"They trade mortgage-backed securities.\",\n",
    "        #         {\n",
    "        #             \"heads\": [1, 1, 4, 4, 5, 1, 1],\n",
    "        #             \"deps\": [\"nsubj\", \"ROOT\", \"compound\", \"punct\", \"nmod\", \"dobj\", \"punct\"],\n",
    "        #         },\n",
    "        #     ),\n",
    "        # ]\n",
    "        \n",
    "        TRAIN_DATA = train_data\n",
    "        \n",
    "        # add the parser to the pipeline if it doesn't exist\n",
    "        # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "        if \"parser\" not in nlp.pipe_names:\n",
    "            parser = nlp.create_pipe(\"parser\")\n",
    "            nlp.add_pipe(parser, first=True)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        else:\n",
    "            parser = nlp.get_pipe(\"parser\")\n",
    "\n",
    "        # add labels to the parser\n",
    "        for _, annotations in TRAIN_DATA:\n",
    "            for dep in annotations.get(\"deps\", []):\n",
    "                parser.add_label(dep)\n",
    "\n",
    "        # get names of other pipes to disable them during training\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"parser\"]\n",
    "        train_losses = []\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train parser\n",
    "            optimizer = nlp.begin_training()\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                losses = {}\n",
    "                # batch up the examples using spaCy's minibatch\n",
    "                batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "                # print(\"Losses\", losses)\n",
    "                train_losses.append(losses)\n",
    "\n",
    "        # test the trained model\n",
    "        test_text = \"I like securities.\"\n",
    "        doc = nlp(test_text)\n",
    "        print(\"Dependencies\", [(t.text, t.dep_, t.head.text) for t in doc])\n",
    "\n",
    "        # save model to output directory\n",
    "        if output_dir is not None:\n",
    "            output_dir = Path(output_dir)\n",
    "            if not output_dir.exists():\n",
    "                output_dir.mkdir()\n",
    "            nlp.to_disk(output_dir)\n",
    "            print(\"Saved model to\", output_dir)\n",
    "            \n",
    "            if test_model:\n",
    "                # test the saved model\n",
    "                print(\"Loading from\", output_dir)\n",
    "                nlp2 = spacy.load(output_dir)\n",
    "                doc = nlp2(test_text)\n",
    "                print(\"Dependencies\", [(t.text, t.dep_, t.head.text) for t in doc])\n",
    "            \n",
    "        return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiMax(object):\n",
    "    \"\"\"\n",
    "    Analyze german text for determining polarity values of \n",
    "    several sentiment associated tokens. The polarity values are calculated \n",
    "    with the german SentiWS-Corpus, enhancing & silencing tokens (* 1.5 / * 0.5)\n",
    "    and negations (* -1.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wordvecs=False, sentiws_path='data/sentiws/', \n",
    "                 polarity_modifiers_path='data/polarity_modifiers.pickle'):\n",
    "        # loading german spacy model\n",
    "        if wordvecs:\n",
    "            self.nlp = spacy.load('de_core_news_md')\n",
    "        else:\n",
    "            self.nlp = spacy.load('de_core_news_sm')\n",
    "        # integrating SentiWS-Corpus as token attribute\n",
    "        sentiws = spaCySentiWS(sentiws_path=sentiws_path)\n",
    "        self.nlp.add_pipe(sentiws)\n",
    "        self.doc = None\n",
    "        self.modifiers = pickle.load(open(polarity_modifiers_path, 'rb'))\n",
    "        if not Token.has_extension(\"modified\"):\n",
    "            Token.set_extension(\"modified\", getter=self.modify_polarity)\n",
    "        if not Token.has_extension(\"negated\"):\n",
    "            Token.set_extension(\"negated\", getter=self.negate)\n",
    "    \n",
    "    def modify_polarity(self, token):\n",
    "        children = token.children\n",
    "        polarity = token._.sentiws\n",
    "        if not polarity:\n",
    "            polarity = 0.0\n",
    "        for child in children:\n",
    "            if child.lower_ in self.modifiers['polarity_enhancer']:\n",
    "                return polarity * 1.5\n",
    "            elif child.lower_ in self.modifiers['polarity_reducer']:\n",
    "                return polarity * 0.5\n",
    "        return polarity\n",
    "    \n",
    "    def negate(self, token):\n",
    "        children = token.children\n",
    "        polarity = token._.modified\n",
    "        for child in children:\n",
    "            if child.dep_ == 'ng':\n",
    "                return -1.0 * polarity\n",
    "        return polarity\n",
    "        \n",
    "    def polarize(self, text):\n",
    "        self.doc = self.nlp(text)\n",
    "    \n",
    "    def sentimax(self):\n",
    "        polarity_dict = {\"token\": [], \"dep\": [], \"sentiws\": [], \"modified\": [], \"negated\": []}\n",
    "        if self.doc:\n",
    "            for token in self.doc:\n",
    "                polarity_dict[\"token\"].append(token.text)\n",
    "                polarity_dict[\"dep\"].append(token.dep_)\n",
    "                polarity_dict[\"sentiws\"].append(token._.sentiws)\n",
    "                polarity_dict[\"modified\"].append(token._.modified)\n",
    "                polarity_dict[\"negated\"].append(token._.negated)\n",
    "                \n",
    "        return pd.DataFrame(data=polarity_dict)           \n",
    "    \n",
    "    def annotate_dependencies(self, texts_to_annotate, display=False):\n",
    "        true_anns = []\n",
    "        false_anns = []\n",
    "        \n",
    "        def format_deps(doc):\n",
    "            deps = {\"heads\": [], \"deps\": []}\n",
    "            for token in doc:\n",
    "                deps[\"heads\"].append(token.head)\n",
    "                deps[\"deps\"].append(token.dep_)\n",
    "            return (doc.text, deps)\n",
    "                    \n",
    "        for i in range(len(texts_to_annotate)):\n",
    "            self.doc = self.nlp(texts_to_annotate[i])\n",
    "            if display:\n",
    "                displacy.render(self.doc, style='dep', jupyter=True)\n",
    "            else:\n",
    "                deps = []\n",
    "                for token in self.doc:\n",
    "                    deps.append(\"({}, {}, {})\".format(token.text, token.dep_, token.head))\n",
    "                print(\" \".join(deps))\n",
    "            user_input = input(\"Is it wrong?: \")\n",
    "            if user_input == '' or user_input == None:\n",
    "                true_anns.append(format_deps(self.doc))\n",
    "            elif user_input.lower() == 'y':\n",
    "                false_anns.append(self.doc.text)\n",
    "            elif user_input.lower() == 'b':\n",
    "                i -= 2\n",
    "            elif user_input.lower() == 'c' or user_input.lower() == 'q':\n",
    "                break\n",
    "        \n",
    "        return true_anns, false_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimax = SentiMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimax.polarize('Google verkündet kein neues Smartphone auf den Markt zu bringen.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google None 0.0 0.0 MISC\n",
      "verkündet None 0.0 0.0 \n",
      "kein None 0.0 0.0 \n",
      "neues 0.004 0.004 0.004 \n",
      "Smartphone None 0.0 0.0 \n",
      "auf None 0.0 0.0 \n",
      "den None 0.0 0.0 \n",
      "Markt None 0.0 0.0 \n",
      "zu None 0.0 0.0 \n",
      "bringen None 0.0 0.0 \n",
      ". None 0.0 0.0 \n"
     ]
    }
   ],
   "source": [
    "for token in sentimax.doc:\n",
    "    print(token.text, token._.sentiws, token._.modified, token._.negated, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>sentiws</th>\n",
       "      <th>modified</th>\n",
       "      <th>negated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>sb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verkündet</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kein</td>\n",
       "      <td>nk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neues</td>\n",
       "      <td>nk</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smartphone</td>\n",
       "      <td>oa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auf</td>\n",
       "      <td>mo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>den</td>\n",
       "      <td>nk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Markt</td>\n",
       "      <td>nk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>zu</td>\n",
       "      <td>pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bringen</td>\n",
       "      <td>oc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token    dep  sentiws  modified  negated\n",
       "0       Google     sb      NaN     0.000    0.000\n",
       "1    verkündet   ROOT      NaN     0.000    0.000\n",
       "2         kein     nk      NaN     0.000    0.000\n",
       "3        neues     nk    0.004     0.004    0.004\n",
       "4   Smartphone     oa      NaN     0.000    0.000\n",
       "5          auf     mo      NaN     0.000    0.000\n",
       "6          den     nk      NaN     0.000    0.000\n",
       "7        Markt     nk      NaN     0.000    0.000\n",
       "8           zu     pm      NaN     0.000    0.000\n",
       "9      bringen     oc      NaN     0.000    0.000\n",
       "10           .  punct      NaN     0.000    0.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimax.sentimax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entitizer(object):\n",
    "    \"\"\"\n",
    "    Find Named-Entities in german texts \n",
    "    as keyword generator for sentimax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wordvecs=False):\n",
    "        # loading german spacy model\n",
    "        if wordvecs:\n",
    "            self.nlp = spacy.load('de_core_news_md')\n",
    "        else:\n",
    "            self.nlp = spacy.load('de_core_news_sm')\n",
    "        self.doc = None\n",
    "    \n",
    "    def find_entities(self, text, visualize=False):\n",
    "        self.doc = self.nlp(text)\n",
    "        if visualize:\n",
    "            displacy.render(self.doc, style='ent', jupyter=True)\n",
    "        entities = [token for token in self.doc if token.ent_type_]\n",
    "        return entities\n",
    "    \n",
    "    def annotate_entities(self, texts_to_annotate):\n",
    "        true_anns = []\n",
    "        false_anns = []\n",
    "        \n",
    "        def format_ents(doc):\n",
    "            ents = {\"entities\": []}\n",
    "            for token in doc:\n",
    "                if token.ent_type_:\n",
    "                    ents[\"entities\"].append((token.idx, token.idx + len(token), token.ent_type_))\n",
    "            return (doc.text, ents)\n",
    "                    \n",
    "        for i in range(len(texts_to_annotate)):\n",
    "            self.doc = self.nlp(texts_to_annotate[i])\n",
    "            displacy.render(self.doc, style='ent', jupyter=True)\n",
    "            user_input = input(\"Is it wrong?: \")\n",
    "            if user_input == '' or user_input == None:\n",
    "                true_anns.append(format_ents(self.doc))\n",
    "            elif user_input.lower() == 'y':\n",
    "                false_anns.append(self.doc.text)\n",
    "            elif user_input.lower() == 'b':\n",
    "                i -= 2\n",
    "            elif user_input.lower() == 'c' or user_input.lower() == 'q':\n",
    "                break\n",
    "        \n",
    "        return true_anns, false_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entitizer = Entitizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = entitizer.find_entities('Google verkündet eine Kooperation mit Mark Zuckerberg einzugehen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pickle.load(open(\"Klinikbewertungen\", 'rb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "true_annotations, false_annotations = entitizer.annotate_entities(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sauber, ROOT, Sauber) (,, punct, Sauber) (freundliche, nk, Mitarbeiter) (Mitarbeiter, cj, Sauber) (und, cd, Mitarbeiter) (gutes, nk, Essen) (Essen, cj, und) (., punct, Sauber) (Überschattet, ROOT, Überschattet) (jedoch, mo, Überschattet) (davon, op, Überschattet) (,, punct, davon) (dass, cp, sind) (mein, nk, Arzt) (behandelnder, nk, Arzt) (Arzt, sb, sind) (mir, da, meinen) (nun, mo, meinen) (seit, mo, meinen) (5, nk, Tagen) (Tagen, nk, seit) (meinen, nk, Befund) (Befund, nk, Arzt) (nicht, ng, kundtut) (kundtut, mo, sind) (obwohl, cp, sind) (meine, nk, MRT) (MRT, sb, sind) (Ergebnisse, nk, MRT) (schon, mo, lange) (lange, mo, bereit) (bereit, pd, sind) (sind, re, davon) (., punct, Überschattet) (Einen, nk, Arztbrief) (Arztbrief, oa, vorlegen) (konnte, ROOT, konnte) (ich, sb, konnte) (meinem, nk, Urologen) (Urologen, da, ebensowenig) (ebensowenig, mo, vorlegen) (vorlegen, oc, konnte) (um, mo, vorlegen) (weitere, nk, Behandlungen) (Behandlungen, nk, um) (in, mo, ziehen) (Betracht, nk, in) (zu, pm, ziehen) (ziehen, oc, konnte) (., punct, konnte) (\n",
      ", , .) (Kein, nk, Gefühl) (schönes, nk, Gefühl) (Gefühl, oa, wissen) (nicht, ng, wissen) (zu, pm, wissen) (wissen, ROOT, wissen) (was, sb, wissen) (mit, mo, ist) (einem, nk, mit) (los, pd, ist) (ist, oc, wissen) (.., punct, wissen)\n",
      "Is it wrong?: q\n"
     ]
    }
   ],
   "source": [
    "true_annotations, false_annotations = sentimax.annotate_dependencies(reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
